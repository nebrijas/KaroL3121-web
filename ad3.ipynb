{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "46aff4e4",
   "metadata": {},
   "source": [
    "# AD3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06dcc704",
   "metadata": {},
   "source": [
    "Esta es la Actividad Dirigida 3 que consiste en hacer un ejercicio de programación literaria aprovechando el código que hemos usado en Programación, con Python.\n",
    "\n",
    "A continuación la actividad:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6217ccb2",
   "metadata": {},
   "source": [
    "### Librerías y módulos \n",
    "Los dividimos en internos y externos\n",
    "#### Módulos internos\n",
    "1. [time:](https://www.solvetic.com/tutoriales/article/366-python-modulo-time/) Contiene funcionalidades que nos permiten, entre otras cosas, manipular y dar formato a fechas y horas.\n",
    "2. [csv:](https://noviello.it/es/como-leer-escribir-y-analizar-csv-en-python/) Tipo de archivo que se utiliza para almacenar datos en una forma tabular estructurada (fila-columna).\n",
    "3. [re:](https://python-para-impacientes.blogspot.com/2014/02/expresiones-regulares.html) Cuenta con funciones para trabajar con expresiones regulares y cadenas.\n",
    "4. [os:](https://www.cosmiclearn.com/lang-es/python-os.php) Proporciona y expone los detalles y la funcionalidad del sistema operativo.\n",
    "\n",
    "#### Librerías externas\n",
    "1. [requests:](https://cosasdedevs.com/posts/web-scraping-con-requests-y-beautifulsoup-en-python/) La utilizaremos para realizar las peticiones a la página de la que vamos a extraer los datos.\n",
    "2. [bs4:](https://es.acervolima.com/python-beautifulsoup-encontrar-todas-las-clases/) Es una biblioteca de Python para extraer datos de archivos HTML y XML.\n",
    "3. [pandas:](https://pandas.pydata.org/) Es una herramienta de manipulación y análisis de datos de código abierto rápida, construida sobre el lenguaje de programación Python.\n",
    "4. [termcolor:](https://ourcodeworld.co/articulos/leer/973/creando-tu-propio-shazam-identificar-canciones-con-python-a-traves-de-huellas-digitales-de-audio-en-ubuntu-1804) Es una biblioteca para imprimir mensajes de colores en el terminal.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e9a58bc",
   "metadata": {},
   "source": [
    "### Instalación de librerías\n",
    "Si la librería viene por defecto con Python no debes instalarla, pero en caso contrario tendrás que hacerlo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6522957c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Defaulting to user installation because normal site-packages is not writeable\n",
      "Requirement already satisfied: requests in c:\\programdata\\anaconda3\\nueva carpeta\\lib\\site-packages (2.27.1)\n",
      "Collecting bs4\n",
      "  Downloading bs4-0.0.1.tar.gz (1.1 kB)\n",
      "Requirement already satisfied: pandas in c:\\programdata\\anaconda3\\nueva carpeta\\lib\\site-packages (1.4.2)\n",
      "Collecting termcolor\n",
      "  Downloading termcolor-1.1.0.tar.gz (3.9 kB)\n",
      "Requirement already satisfied: charset-normalizer~=2.0.0 in c:\\programdata\\anaconda3\\nueva carpeta\\lib\\site-packages (from requests) (2.0.4)\n",
      "Requirement already satisfied: idna<4,>=2.5 in c:\\programdata\\anaconda3\\nueva carpeta\\lib\\site-packages (from requests) (3.3)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in c:\\programdata\\anaconda3\\nueva carpeta\\lib\\site-packages (from requests) (2021.10.8)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in c:\\programdata\\anaconda3\\nueva carpeta\\lib\\site-packages (from requests) (1.26.9)\n",
      "Requirement already satisfied: beautifulsoup4 in c:\\programdata\\anaconda3\\nueva carpeta\\lib\\site-packages (from bs4) (4.11.1)\n",
      "Requirement already satisfied: numpy>=1.18.5 in c:\\programdata\\anaconda3\\nueva carpeta\\lib\\site-packages (from pandas) (1.21.5)\n",
      "Requirement already satisfied: python-dateutil>=2.8.1 in c:\\programdata\\anaconda3\\nueva carpeta\\lib\\site-packages (from pandas) (2.8.2)\n",
      "Note: you may need to restart the kernel to use updated packages.Requirement already satisfied: pytz>=2020.1 in c:\\programdata\\anaconda3\\nueva carpeta\\lib\\site-packages (from pandas) (2021.3)\n",
      "Requirement already satisfied: six>=1.5 in c:\\programdata\\anaconda3\\nueva carpeta\\lib\\site-packages (from python-dateutil>=2.8.1->pandas) (1.16.0)\n",
      "Requirement already satisfied: soupsieve>1.2 in c:\\programdata\\anaconda3\\nueva carpeta\\lib\\site-packages (from beautifulsoup4->bs4) (2.3.1)\n",
      "Building wheels for collected packages: bs4, termcolor\n",
      "  Building wheel for bs4 (setup.py): started\n",
      "  Building wheel for bs4 (setup.py): finished with status 'done'\n",
      "  Created wheel for bs4: filename=bs4-0.0.1-py3-none-any.whl size=1272 sha256=2e4105dbac268860946c223fd62dab4b2ad9bf74d54d2d0511f4aaea8e4e6773\n",
      "  Stored in directory: c:\\users\\karol\\appdata\\local\\pip\\cache\\wheels\\73\\2b\\cb\\099980278a0c9a3e57ff1a89875ec07bfa0b6fcbebb9a8cad3\n",
      "  Building wheel for termcolor (setup.py): started\n",
      "  Building wheel for termcolor (setup.py): finished with status 'done'\n",
      "  Created wheel for termcolor: filename=termcolor-1.1.0-py3-none-any.whl size=4848 sha256=13d9c754d93a590033ae879d66cfdc824e3ae9e8c5cd78e97ed4d7701b667e9b\n",
      "\n",
      "  Stored in directory: c:\\users\\karol\\appdata\\local\\pip\\cache\\wheels\\b6\\0d\\90\\0d1bbd99855f99cb2f6c2e5ff96f8023fad8ec367695f7d72d\n",
      "Successfully built bs4 termcolor\n",
      "Installing collected packages: termcolor, bs4\n",
      "Successfully installed bs4-0.0.1 termcolor-1.1.0\n"
     ]
    }
   ],
   "source": [
    "pip install requests bs4 pandas termcolor"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "681f6144",
   "metadata": {},
   "source": [
    "### Importación de librerías\n",
    "Además de las funcionalidades básicas, Python cuenta con una serie de bibliotecas que ya fueron programadas y se pueden importar a mi programa principal para usarlas."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "dda3a38e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "502dd667",
   "metadata": {},
   "source": [
    "### Resultados\n",
    "Aquí irán los datos luego de hacer el scrapping. \n",
    "`resultados = []`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25ea7bff",
   "metadata": {},
   "source": [
    "### Método de trabajo\n",
    "Elegiré  partes del código que que parezcan interesantes a fin de ejecutarlas. En el primer segmento del código elegido se solicitan los titulares de la sección de opinión del diario El País. En este caso la estructura identificada para los títulos en HTML es h2. La petición a la web se hace con  el método\n",
    "`req = requests.get(+URL)`. El condicional `if (req.status_code != 200)` indica que si  el estatus code no es 200 no se puede leer la página. Luego pasamos el contenido HTML de la web a un objeto BeautifulSoup `(req3.text, 'html.parser')`. Después arroja  todos los h2 que están las entradas y los imprimimos.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "be7dc408",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nuevo paquete anticrisis...\n",
      "…porque la guerra de Putin es imprevisible\n",
      "El castellano es el fracaso \n",
      "Parole, parole, parole…\n",
      "Vidas gitanas\n",
      "El leninismo reaccionario ha ganado\n",
      "Cierra las piernas, niña\n",
      "El aborto y la guerra fría\n",
      "Si la OTAN no existiera \n",
      "América Latina, ‘quo vadis?’\n",
      "Perder el sentido de la oportunidad\n",
      "Boca abierta\n",
      "Aborto en EE UU: medio siglo hacia atrás\n",
      "El Roto\n",
      "Peridis\n",
      "Flavita Banana\n",
      "Riki Blanco\n",
      "Sciammarella\n",
      "Planeta de Plástico, por Malagón\n",
      "Envía tu carta\n",
      "Réquiem por la sierra de la Culebra  \n",
      "Harta de los negacionistas\n",
      "Desmantelando la sanidad, que es gerundio\n",
      "Opinar sin insultar\n",
      "Contra el conflicto de intereses, transparencia\n",
      "Entre los derechos de Esther López y los de los lectores\n",
      "El defensor del lector contesta\n"
     ]
    }
   ],
   "source": [
    "req3 = requests.get(\"https://elpais.com/opinion\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup3 = BeautifulSoup(req3.text, 'html.parser')\n",
    " \n",
    "tags = soup3.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    \n",
    "    \n",
    " \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73a0cfc8",
   "metadata": {},
   "source": [
    "### Parte  2 del código \n",
    "Este código tiene  solicitudes y métodos iguales al anterior `(requests.get, if req.status_code != 200, raise Exception, req16.text, 'html.parser', soup16.findAll)`, pero los titulares que se desean son de la sección de educación. También la estructura identificada es h2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "5970ab16",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Primero de carrera: los bachilleres de la escuela pública son los que más materias aprueban \n",
      "Universidad en reconstrucción \n",
      "Volver a la escuela en América Latina tras dos años de pandemia\n",
      "El intento de suicidio de Juan y las somatizaciones de Carmen: ¿por qué los colegios necesitan un psicólogo educativo?\n",
      "El nuevo sistema para evaluar los conocimientos digitales de los profesores valdrá en toda España\n",
      "Un aula de Vallecas se rebela contra los libros de texto que silencian a las mujeres\n",
      "Solo un 3% de alumnos de Cataluña pidió hacer el examen de Selectividad en castellano\n",
      "Subirats reinterpreta la ley de Universidades: freno a la precariedad, facilidades para los alumnos extranjeros y ciencia abierta\n",
      "“Los profesores no van a cambiar de golpe su forma de trabajar el curso que viene por la nueva ley educativa”\n",
      "Trampantojo de pescado y lasaña vegetal en el comedor: cientos de colegios buscan fórmulas para que llevar la alimentación responsable a los niños\n",
      "Vídeo | Las recetas del cocinero escolar para que los niños de El Grau coman sano y rico \n",
      "Sin currículos\n",
      "La escuela concertada ante las desigualdades: el debate pendiente\n",
      "La equidad frente a las políticas educativas de privatización en Andalucía\n",
      "No hay lunes al sol en la Universidad\n",
      "Ofrecer comedor gratis en todos los colegios públicos es “alcanzable y urgente”: costaría 1.664 millones al año, según la ONG Educo  \n",
      "Una fórmula para que la escuela pública compita mejor con la concertada\n",
      "La pérdida de alumnos por el descenso de la natalidad está afectando con más fuerza a la escuela pública que a la concertada\n",
      "El Ayuntamiento de Madrid guarda en un cajón una herramienta ya pagada para ver los datos de contaminación al detalle \n",
      "La implantación del nuevo Bachillerato general fracasa pese a su demanda potencial: “Creímos que lo pedirían seis alumnos y lo han hecho 27”\n",
      "Toni Solano, director de instituto: “Veo mal a los niños, necesitan muchísima ayuda”\n",
      "Niños, Administraciones y redes sociales: prohibir su uso con una mano y enseñar a crear contenidos con la otra\n",
      "El Gobierno aprueba el decreto de bachillerato: los alumnos podrán terminar con un suspenso y desembocará en una nueva Selectividad\n",
      "La ley del silencio dentro y fuera del aula\n",
      "Las universitarias desertan del grado de Matemáticas ahora que tiene pleno empleo. ¿Por qué?\n",
      "Golpe a la temporalidad en las universidades: 25.000 profesores asociados serán indefinidos a tiempo parcial\n",
      "Antonio Abril: “Yo le decía a Castells: ‘Tienes que aguantar la presión, tienes que hacer la reforma universitaria”\n",
      "Los universitarios extranjeros podrán quedarse un año en España automáticamente al terminar la carrera\n"
     ]
    }
   ],
   "source": [
    "req7 = requests.get(\"https://elpais.com/educacion/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup7 = BeautifulSoup(req7.text, 'html.parser')\n",
    " \n",
    "tags = soup7.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8fc5654b",
   "metadata": {},
   "source": [
    "### Otra muestra de solicitud de titulares\n",
    "Los titulares (h2) extraídos en esta oportunidad corresponden al apartado de televisión. En primera instancia se hizo la petición con `requests.get`, luego el condicional `if (req.status_code != 200)` advierte que si el estatus code no es igual a 200 no se puede leer la página. El raise Exception recuerda que no se puede hacer Web Scraping en\"+ URL. Con la siguiente acción `(req.text, \"html.parser\")` pasamos el contenido HTML de la web a un objeto BeautifulSoup. Y después obtenemos los h2 y los imprimimos.\n",
    "\n",
    "**Elegí estas tres  porque la solicitud se repite en el resto de las secciones, excepto por los pedidos de color al final.**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d15cdb6f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "24 series para este verano\n",
      "¿Merece la pena ver ‘La casa de papel: Corea’? Diferencias y semejanzas con la serie original\n",
      "Pepón Nieto denuncia que TVE vetó una de sus películas en ‘Cine de barrio’ y la corporación recula: se emitirá este sábado\n",
      "El espíritu de Balbín está por todas partes\n",
      "Los Romanoffs\n",
      "Maria Ressa, la periodista filipina “no exenta de ser asesinada”\n",
      "‘Supervivientes’ y la incultura general\n",
      "‘La clave’ de Balbín, gloria y caída del programa que cambió la televisión española\n",
      "Un viaje en el tiempo con Kurt Vonnegut\n",
      "Muere José Luis Balbín, mítico presentador y creador del programa ‘La clave’\n",
      "El TSJ de Madrid declara nulo el despido del guionista de TVE que escribió el rótulo “Leonor se va de España, como su abuelo”\n",
      "‘Legacy’: un Bosch sin placa y un poco perdido\n",
      "Claves de la nueva Ley General de Comunicación Audiovisual, más allá del enfado de los productores independientes\n",
      "‘Sanditon’ resucita a Jane Austen\n",
      "Ciencia, tecnología y entrevistas en las novedades de la programación de verano en la Cadena SER\n",
      "Los abonados a las plataformas y televisiones de pago superan el número de hogares españoles \n",
      "Mueren en un accidente de tráfico dos actores mexicanos durante el rodaje de la serie de Netflix ‘El Elegido’  \n",
      "¿Qué ver hoy en TV? Domingo 26 de junio de 2022\n",
      "Nueve capítulos para recordar ‘The Wire’ en su 20º aniversario\n",
      "Harry Palmer: el tercer vértice del mágico triángulo de espías británicos\n",
      "Las series de junio de 2022: ‘The Boys’ en Amazon Prime Video; ‘Peaky Blinders’ en Netflix y otras\n",
      "La fuerza acompaña a ‘Obi-Wan Kenobi’, una serie deslumbrante\n"
     ]
    }
   ],
   "source": [
    "req16 = requests.get(\"https://elpais.com/television/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup16 = BeautifulSoup(req16.text, 'html.parser')\n",
    " \n",
    "tags = soup16.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f1048384",
   "metadata": {},
   "source": [
    "### Palabras clave y color\n",
    "Esta búsqueda tiene por objetivo segmentar las noticias de acuerdo con las palabras clave, a las cuales se les da un color (verde). Para ello se emplea principalmente `print` (imprimir) y `str_match.` Esta última es una función que se usa para determinar si cada cadena en los datos subyacentes del objeto de serie dado coincide con una expresión regular."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "d13088ee",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m\u001b[34mA continuación se muestran los titulares de las páginas principales del diario El País que contienen las siguientes palabras:\u001b[0m\n",
      "\u001b[1m\u001b[32mFeminismo\u001b[0m\n",
      "Americanas: Reportajes y noticias sobre feminismo e historias con enfoque de género de la región\n",
      "\u001b[1m\u001b[32mIgualdad\u001b[0m\n",
      "Un año de eutanasia en España: 172 casos y una gran desigualdad entre las comunidades autónomas\n",
      "La escuela concertada ante las desigualdades: el debate pendiente\n",
      "\u001b[1m\u001b[32mMujeres\u001b[0m\n",
      "Un día muy triste para todas las mujeres\n",
      "El Supremo de Estados Unidos destruye un derecho fundamental de las mujeres\n",
      "Un día muy triste para todas las mujeres\n",
      "Un aula de Vallecas se rebela contra los libros de texto que silencian a las mujeres\n",
      "Sharon Stone: “Perdí nueve hijos por abortos espontáneos. Las mujeres no tenemos un espacio donde discutir la profundidad de estas pérdidas”\n",
      "\u001b[1m\u001b[32mMujer\u001b[0m\n",
      "Ellos no bailaron solos: la mujer detrás de la loca historia de Locomía\n",
      "Un día muy triste para todas las mujeres\n",
      "El Supremo de Estados Unidos destruye un derecho fundamental de las mujeres\n",
      "Un día muy triste para todas las mujeres\n",
      "El conde de Atarés asesinó a su mujer el día en que se disponía a abandonarle\n",
      "Un aula de Vallecas se rebela contra los libros de texto que silencian a las mujeres\n",
      "Sharon Stone: “Perdí nueve hijos por abortos espontáneos. Las mujeres no tenemos un espacio donde discutir la profundidad de estas pérdidas”\n",
      "Siri, Alexa, Sophia: ¿por qué asistentes virtuales y humanoides tienen nombre de mujer?\n",
      "\u001b[1m\u001b[32mBrecha salarial\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[32mMachismo\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[32mViolencia\u001b[0m\n",
      "Epidemia de violencia: claves del negocio de las armas en Estados Unidos\n",
      "\u001b[1m\u001b[32mMaltrato\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[32mHomicidio\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[32mGénero\u001b[0m\n",
      "Americanas: Reportajes y noticias sobre feminismo e historias con enfoque de género de la región\n",
      "\u001b[1m\u001b[32mAsesinato\u001b[0m\n",
      "\n",
      "\u001b[1m\u001b[32mSexo\u001b[0m\n",
      "Un libro y un documental revisitan la figura de George Michael: atracones de helado, culebrones, sexo y éxtasis\n",
      "El mejor sexo de tu vida\n",
      "Un libro y un documental revisitan la figura de George Michael: atracones de helado, culebrones, sexo y éxtasis\n"
     ]
    }
   ],
   "source": [
    "os.system(\"clear\")\n",
    " \n",
    "print(colored(\"A continuación se muestran los titulares de las páginas principales del diario El País que contienen las siguientes palabras:\", 'blue', attrs=['bold']))\n",
    "print(colored(\"Feminismo\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"feminismo\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Igualdad\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"igualdad\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Mujeres\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"mujeres\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Mujer\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"mujer\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Brecha salarial\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"brecha salarial\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Machismo\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"machismo\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Violencia\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"violencia\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Maltrato\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"maltrato\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Homicidio\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"homicidio\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Género\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"género\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Asesinato\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"asesinato\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Sexo\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"sexo\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fef42bf",
   "metadata": {},
   "source": [
    "## Código fuente del ejercicio\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "72178904",
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "import csv\n",
    "import re\n",
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import pandas as pd\n",
    "from termcolor import colored\n",
    " \n",
    "resultados = []\n",
    " \n",
    "req = requests.get(\"https://resultados.elpais.com\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup = BeautifulSoup(req.text, 'html.parser')\n",
    " \n",
    "tags = soup.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req2 = requests.get(\"https://elpais.com/internacional\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup2 = BeautifulSoup(req2.text, 'html.parser')\n",
    " \n",
    "tags = soup2.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req3 = requests.get(\"https://elpais.com/opinion\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup3 = BeautifulSoup(req3.text, 'html.parser')\n",
    " \n",
    "tags = soup3.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req4 = requests.get(\"https://elpais.com/espana/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup4 = BeautifulSoup(req4.text, 'html.parser')\n",
    " \n",
    "tags = soup4.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req5 = requests.get(\"https://elpais.com/economia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup5 = BeautifulSoup(req5.text, 'html.parser')\n",
    " \n",
    "tags = soup5.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req6 = requests.get(\"https://elpais.com/sociedad/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup6 = BeautifulSoup(req6.text, 'html.parser')\n",
    " \n",
    "tags = soup6.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req7 = requests.get(\"https://elpais.com/educacion/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup7 = BeautifulSoup(req7.text, 'html.parser')\n",
    " \n",
    "tags = soup7.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req8 = requests.get(\"https://elpais.com/clima-y-medio-ambiente/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup8 = BeautifulSoup(req8.text, 'html.parser')\n",
    " \n",
    "tags = soup8.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req9 = requests.get(\"https://elpais.com/ciencia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup9 = BeautifulSoup(req9.text, 'html.parser')\n",
    " \n",
    "tags = soup9.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req10 = requests.get(\"https://elpais.com/cultura/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup10 = BeautifulSoup(req10.text, 'html.parser')\n",
    " \n",
    "tags = soup10.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req11 = requests.get(\"https://elpais.com/babelia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup11 = BeautifulSoup(req11.text, 'html.parser')\n",
    " \n",
    "tags = soup11.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req12 = requests.get(\"https://elpais.com/deportes/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup12 = BeautifulSoup(req12.text, 'html.parser')\n",
    " \n",
    "tags = soup12.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req13 = requests.get(\"https://elpais.com/tecnologia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup13 = BeautifulSoup(req13.text, 'html.parser')\n",
    " \n",
    "tags = soup13.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req14 = requests.get(\"https://elpais.com/tecnologia/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup14 = BeautifulSoup(req14.text, 'html.parser')\n",
    " \n",
    "tags = soup14.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req15 = requests.get(\"https://elpais.com/gente/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup15 = BeautifulSoup(req15.text, 'html.parser')\n",
    " \n",
    "tags = soup15.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req16 = requests.get(\"https://elpais.com/television/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup16 = BeautifulSoup(req16.text, 'html.parser')\n",
    " \n",
    "tags = soup16.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    "req17 = requests.get(\"https://elpais.com/eps/\")\n",
    "# Si el estatus code no es 200 no se puede leer la página\n",
    "if (req.status_code != 200):\n",
    " raise Exception(\"No se puede hacer Web Scraping en\"+ URL)\n",
    "soup17 = BeautifulSoup(req17.text, 'html.parser')\n",
    " \n",
    "tags = soup17.findAll(\"h2\")\n",
    " \n",
    "for h2 in tags:\n",
    "    print(h2.text)\n",
    "    resultados.append(h2.text)\n",
    " \n",
    " \n",
    "os.system(\"clear\")\n",
    " \n",
    "print(colored(\"A continuación se muestran los titulares de las páginas principales del diario El País que contienen las siguientes palabras:\", 'blue', attrs=['bold']))\n",
    "print(colored(\"Feminismo\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"feminismo\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Igualdad\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"igualdad\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Mujeres\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"mujeres\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Mujer\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"mujer\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Brecha salarial\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"brecha salarial\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Machismo\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"machismo\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Violencia\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"violencia\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Maltrato\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"maltrato\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Homicidio\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"homicidio\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Género\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"género\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Asesinato\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"asesinato\" in s]\n",
    "print(\"\\n\".join(str_match))\n",
    " \n",
    "print(colored(\"Sexo\", 'green', attrs=['bold']))\n",
    " \n",
    "str_match = [s for s in resultados if \"sexo\" in s]\n",
    "print(\"\\n\".join(str_match))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
